{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505166ea",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f3d3d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "from Map import MapEnv \n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from gymnasium.utils.env_checker import check_env\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb5a022",
   "metadata": {},
   "source": [
    "# Training and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e3dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DriverAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float,\n",
    "    ):\n",
    "        self.env = env\n",
    "        # Q-table\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def _obs_to_tuple(self, obs):\n",
    "        d = obs[\"driver\"]\n",
    "        dest = obs[\"destination\"]\n",
    "        return (int(d[0]), int(d[1]), int(dest[0]), int(dest[1]))\n",
    "\n",
    "    def choose_action(self, obs: tuple[int, int, bool]) -> int: \n",
    "        obs_tuple = self._obs_to_tuple(obs)\n",
    "        # Exploration\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        # Exploitation\n",
    "        else :\n",
    "            return int(np.argmax(self.q_values[obs_tuple]))\n",
    "        \n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        obs_tuple = self._obs_to_tuple(obs)\n",
    "        next_obs_tuple = self._obs_to_tuple(next_obs)\n",
    "        \n",
    "        # Decide next best action\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs_tuple])\n",
    "\n",
    "        # Bellman Equation to update Q-value\n",
    "        destination = reward + self.discount_factor * future_q_value\n",
    "\n",
    "        # How wrong was our current estimate?\n",
    "        temporal_difference = destination - self.q_values[obs_tuple][action]\n",
    "\n",
    "        # Update q-value in direction of error\n",
    "        self.q_values[obs_tuple][action] = (\n",
    "            self.q_values[obs_tuple][action] + self.lr * temporal_difference\n",
    "        )\n",
    "\n",
    "        # Track learning progress\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n",
    "\n",
    "    # def test_agent(self, env, n_episodes):\n",
    "    #     total_rewards = []\n",
    "    #     successes = []\n",
    "    #     episode_lengths = []\n",
    "\n",
    "    #     # Disable exploration for testing\n",
    "    #     old_epsilon = self.epsilon\n",
    "    #     self.epsilon = 0.0\n",
    "\n",
    "    #     for _ in range(n_episodes):\n",
    "    #         obs, info = env.reset()\n",
    "    #         episode_reward = 0\n",
    "    #         steps = 0\n",
    "    #         done = False\n",
    "\n",
    "    #         while not done:\n",
    "    #             action = self.choose_action(obs)\n",
    "    #             obs, reward, terminated, truncated, info = env.step(action)\n",
    "    #             episode_reward += reward\n",
    "    #             steps += 1\n",
    "    #             done = terminated or truncated\n",
    "\n",
    "    #         total_rewards.append(episode_reward)\n",
    "    #         successes.append(terminated) \n",
    "    #         episode_lengths.append(steps)\n",
    "\n",
    "    #     # Restore original epsilon\n",
    "    #     self.epsilon = old_epsilon\n",
    "\n",
    "    #     success_rate = np.mean(successes)\n",
    "    #     average_reward = np.mean(total_rewards)\n",
    "    #     average_length = np.mean(episode_lengths)\n",
    "\n",
    "    #     tqdm.write(f\"Test Results over {n_episodes} episodes:\")\n",
    "    #     tqdm.write(f\"Success Rate: {success_rate:.1%}\")\n",
    "    #     tqdm.write(f\"Average Reward: {average_reward:.3f}\")\n",
    "    #     tqdm.write(f\"Average Episode Length: {average_length:.1f}\")\n",
    "    #     tqdm.write(f\"Std Episode Length: {np.std(episode_lengths):.1f}\")\n",
    "\n",
    "\n",
    "    def train_agent(agent, env, n_episodes: int, log_interval: int=5000):\n",
    "        episode_rewards = []\n",
    "        episode_lengths = []\n",
    "        epsilon_history = []\n",
    "        success_rate_history = []\n",
    "\n",
    "        print(f\"\\nTraining for {n_episodes} episodes...\\n\")\n",
    "\n",
    "        for episode in tqdm(range(n_episodes), desc=\"Training\"):\n",
    "            obs, info = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "\n",
    "            while not done:\n",
    "                # Choose action using epsilon-greedy policy\n",
    "                action = agent.choose_action(obs)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "                # Q-learning update (uses max over next state, not next action)\n",
    "                agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                done = terminated or truncated\n",
    "                obs = next_obs\n",
    "\n",
    "            agent.decay_epsilon()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_lengths.append(steps)\n",
    "            epsilon_history.append(agent.epsilon)\n",
    "\n",
    "            if episode >= 99:\n",
    "                recent_successes = [1 if r > 0 else 0 for r in episode_rewards[-100:]]\n",
    "                success_rate_history.append(np.mean(recent_successes))\n",
    "\n",
    "            if (episode + 1) % log_interval == 0:\n",
    "                recent_rewards = episode_rewards[-1000:]\n",
    "                tqdm.write(f\"\\nEpisode {episode + 1}/{n_episodes}\")\n",
    "                tqdm.write(f\"  Avg Reward: {np.mean(recent_rewards):.2f}\")\n",
    "                tqdm.write(f\"  Success Rate: {np.mean(recent_successes):.1%}\")\n",
    "                tqdm.write(f\"  Avg Steps: {np.mean(episode_lengths[-1000:]):.1f}\")\n",
    "                tqdm.write(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "                tqdm.write(f\"  States: {len(agent.q_values)}\")\n",
    "\n",
    "        # Success rate over all episodes\n",
    "        overall_success_rate = np.mean([1 if r > 0 else 0 for r in episode_rewards])\n",
    "\n",
    "        print(f\"\\nTraining complete. States explored: {len(agent.q_values)}\")\n",
    "        print(f\"Final epsilon: {agent.epsilon:.4f}\")\n",
    "        print(f\"Overall Success Rate: {overall_success_rate:.1%}\")\n",
    "\n",
    "        return {\n",
    "            \"episode_rewards\": episode_rewards,\n",
    "            \"episode_lengths\": episode_lengths,\n",
    "            \"epsilon_history\": epsilon_history,\n",
    "            \"success_rate_history\": success_rate_history,\n",
    "            \"overall_success_rate\": overall_success_rate,\n",
    "        }   \n",
    "\n",
    "\n",
    "    def test_agent(agent, env, n_episodes: int = 100, success_threshold: float = 0):\n",
    "        total_rewards = []\n",
    "        successes = []\n",
    "        episode_lengths = []\n",
    "\n",
    "        old_epsilon = agent.epsilon\n",
    "        agent.epsilon = 0.0  # no exploration\n",
    "\n",
    "        for _ in tqdm(range(n_episodes), desc=\"Testing\"):\n",
    "            obs, info = env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                action = agent.choose_action(obs)\n",
    "                obs, reward, terminated, truncated, info = env.step(action)\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                done = terminated or truncated\n",
    "\n",
    "            total_rewards.append(episode_reward)\n",
    "            successes.append(1 if episode_reward > success_threshold else 0)\n",
    "            episode_lengths.append(steps)\n",
    "\n",
    "        # restore epsilon\n",
    "        agent.epsilon = old_epsilon\n",
    "\n",
    "        results = {\n",
    "            \"success_rate\": np.mean(successes),\n",
    "            \"avg_reward\": np.mean(total_rewards),\n",
    "            \"std_reward\": np.std(total_rewards),\n",
    "            \"avg_length\": np.mean(episode_lengths),\n",
    "            \"std_length\": np.std(episode_lengths),\n",
    "            \"rewards\": total_rewards,\n",
    "            \"lengths\": episode_lengths,\n",
    "        }\n",
    "\n",
    "        # Optional summary print\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"TEST RESULTS ({n_episodes} episodes)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Success Rate: {results['success_rate']:.1%}\")\n",
    "        print(f\"Average Reward: {results['avg_reward']:.3f} ± {results['std_reward']:.3f}\")\n",
    "        print(f\"Average Episode Len: {results['avg_length']:.1f} ± {results['std_length']:.1f}\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def visualize_testing_progress(agent, env_class, n_episodes=10):\n",
    "        render_env = env_class(render_mode=\"human\")\n",
    "        old_epsilon = agent.epsilon\n",
    "        agent.epsilon = 0.0  # pure exploitation mode\n",
    "\n",
    "        for episode in range(n_episodes):\n",
    "            obs, info = render_env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            done = False\n",
    "\n",
    "            print(f\"\\n=== Episode {episode + 1} ===\")\n",
    "            print(f\"Driver at: {obs['driver']}, Destination at: {obs['destination']}\")\n",
    "\n",
    "            while not done:\n",
    "                action = agent.choose_action(obs)\n",
    "                obs, reward, terminated, truncated, info = render_env.step(action)\n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                done = terminated or truncated\n",
    "\n",
    "            print(f\"Episode finished in {steps} steps\")\n",
    "            print(f\"Total reward: {episode_reward:.2f}\")\n",
    "            print(f\"Success: {'Yes' if terminated else 'No'}\")\n",
    "\n",
    "        agent.epsilon = old_epsilon\n",
    "        render_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b03e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_avgs(arr, window, convolution_mode):\n",
    "        return np.convolve(\n",
    "            np.array(arr).flatten(),\n",
    "            np.ones(window),\n",
    "            mode=convolution_mode\n",
    "        ) / window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1b22072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(results, agent, window=100, algo_name='Q-learning', save_path=None):\n",
    "\n",
    "    episode_rewards = results[\"episode_rewards\"]\n",
    "    episode_lengths = results[\"episode_lengths\"]\n",
    "    success_rate_history = results[\"success_rate_history\"]\n",
    "    # overall_success_rate = results.get(\"overall_success_rate\", None)\n",
    "\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(18, 14))\n",
    "    fig.suptitle(f'{algo_name} Training Metrics', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # Helper for moving average\n",
    "    def get_moving_avgs(data, window=100, mode='valid'):\n",
    "        return np.convolve(data, np.ones(window)/window, mode=mode)\n",
    "\n",
    "    # 1. Episode Rewards\n",
    "    ax = axes[0, 0]\n",
    "    rewards_ma = get_moving_avgs(episode_rewards, window, 'valid')\n",
    "    ax.plot(episode_rewards, alpha=0.2, color='blue', label='Raw')\n",
    "    ax.plot(range(window-1, len(episode_rewards)), rewards_ma,\n",
    "            color='blue', linewidth=2, label=f'{window}-ep MA')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Total Reward')\n",
    "    ax.set_title('Episode Rewards')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. Episode Lengths\n",
    "    ax = axes[0, 1]\n",
    "    lengths_ma = get_moving_avgs(episode_lengths, window, 'valid')\n",
    "    ax.plot(episode_lengths, alpha=0.2, color='green', label='Raw')\n",
    "    ax.plot(range(window-1, len(episode_lengths)), lengths_ma, color='green', linewidth=2, label=f'{window}-ep MA')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title('Episode Lengths')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    # 3. Success Rate\n",
    "    ax = axes[0, 2]\n",
    "    ax.plot(range(window-1, window-1 + len(success_rate_history)),\n",
    "            np.array(success_rate_history)*100,\n",
    "            color='orange', linewidth=2.5, label=f'Rolling Success Rate ({window}ep)')\n",
    "    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5, linewidth=1, label='Perfect (100%)')\n",
    "    ax.axhline(y=71, color='red', linestyle=':', alpha=0.7, linewidth=1.5, label='Test Result (71%)')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Success Rate (%)')\n",
    "    ax.set_title(f'Success Rate During Training ({window}-ep window)')\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if len(success_rate_history) > 0:\n",
    "        final_success = success_rate_history[-1]\n",
    "        ax.text(0.98, 0.02, f'Final: {final_success:.1f}%',\n",
    "                transform=ax.transAxes, ha='right', va='bottom',\n",
    "                fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "\n",
    "    # 4. Epsilon Decay\n",
    "    ax = axes[1, 0]\n",
    "    epsilon_history = results.get(\"epsilon_history\", [])\n",
    "\n",
    "    if len(epsilon_history) > 0:\n",
    "        ax.plot(epsilon_history, color='orange', linewidth=2)\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Epsilon (ε)')\n",
    "        ax.set_title('Exploration Rate Decay')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No epsilon history\", ha='center', va='center',\n",
    "                fontsize=10, color='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # 5. TD Errors (Per Episode)\n",
    "    ax = axes[1, 1]\n",
    "    td_errors = getattr(agent, \"training_error\", [])\n",
    "    if len(td_errors) > 0:\n",
    "        if len(td_errors) > window:\n",
    "            td_ma = np.convolve(td_errors, np.ones(window)/window, mode='valid')\n",
    "            ax.plot(td_errors, alpha=0.2, color='red', label='Raw')\n",
    "            ax.plot(range(window-1, len(td_errors)), td_ma,\n",
    "                    color='red', linewidth=2, label=f'{window}-ep MA')\n",
    "        else:\n",
    "            ax.plot(td_errors, color='red')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Mean |TD Error|')\n",
    "        ax.set_title('Learning Progress (TD Errors)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No TD error data\", ha='center', va='center', fontsize=10, color='gray')\n",
    "        ax.axis('off')\n",
    "\n",
    "    # 6. Reward Distribution\n",
    "    ax = axes[1, 2]\n",
    "    recent_rewards = episode_rewards[-1000:]\n",
    "    ax.hist(recent_rewards, bins=50, color='teal', alpha=0.7, edgecolor='black')\n",
    "    mean_reward = np.mean(recent_rewards)\n",
    "    ax.axvline(mean_reward, color='red', linestyle='--',\n",
    "               linewidth=2.5, label=f'Mean: {mean_reward:.1f}')\n",
    "    ax.axvline(0, color='gray', linestyle=':', linewidth=1.5, alpha=0.7, label='Zero')\n",
    "    ax.set_xlabel('Total Reward')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title('Reward Distribution (Last 1000 eps)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 7. Cumulative Reward\n",
    "\n",
    "    ax = axes[2, 1]\n",
    "    cumulative_rewards = np.cumsum(episode_rewards)\n",
    "    ax.plot(cumulative_rewards, color='purple', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Cumulative Reward')\n",
    "    ax.set_title('Cumulative Reward Over Episodes')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    else:\n",
    "        plt.savefig(f'{algo_name.lower()}_training_metrics.png', dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {algo_name.lower()}_training_metrics.png\")\n",
    "\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac43c2ff",
   "metadata": {},
   "source": [
    "#  Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da828517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#     n_episodes = 20000\n",
    "#     env = MapEnv(render_mode=None)\n",
    "\n",
    "#     agent = DriverAgent(\n",
    "#         env=env,\n",
    "#         learning_rate=0.1,\n",
    "#         initial_epsilon=1.0,\n",
    "#         epsilon_decay= 1e-5,\n",
    "#         final_epsilon=0.01,\n",
    "#         discount_factor=0.95,\n",
    "#     )\n",
    "\n",
    "#     # TRAINING AGENT\n",
    "#     for episode in range(n_episodes):\n",
    "#         obs, info = env.reset()\n",
    "#         done = False\n",
    "#         while not done:\n",
    "#             # 1. Choose action\n",
    "#             action = agent.choose_action(obs)\n",
    "\n",
    "#             # 2. Take action\n",
    "#             next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "#             # 3. Learn from experience & update Q-values\n",
    "#             agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "#             # 4. Move to next state\n",
    "#             done = terminated or truncated\n",
    "#             obs = next_obs\n",
    "#         # Take less random actions over time\n",
    "#         agent.decay_epsilon()\n",
    "\n",
    "#     # Run evaluation\n",
    "#     agent.test_agent(env, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f205389e",
   "metadata": {},
   "source": [
    "# Testing and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68852f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Driver at: [2 4], Destination at: [5 1]\n",
      "Episode finished in 6 steps\n",
      "Total reward: 99.87\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 2 ===\n",
      "Driver at: [3 5], Destination at: [1 2]\n",
      "Episode finished in 5 steps\n",
      "Total reward: 99.92\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 3 ===\n",
      "Driver at: [4 2], Destination at: [7 3]\n",
      "Episode finished in 4 steps\n",
      "Total reward: 99.95\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 4 ===\n",
      "Driver at: [0 7], Destination at: [4 0]\n",
      "Episode finished in 11 steps\n",
      "Total reward: 99.59\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 5 ===\n",
      "Driver at: [6 6], Destination at: [6 7]\n",
      "Episode finished in 1 steps\n",
      "Total reward: 100.00\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 6 ===\n",
      "Driver at: [4 0], Destination at: [4 1]\n",
      "Episode finished in 1 steps\n",
      "Total reward: 100.00\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 7 ===\n",
      "Driver at: [5 7], Destination at: [5 1]\n",
      "Episode finished in 6 steps\n",
      "Total reward: 99.85\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 8 ===\n",
      "Driver at: [4 6], Destination at: [1 5]\n",
      "Episode finished in 4 steps\n",
      "Total reward: 99.95\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 9 ===\n",
      "Driver at: [1 7], Destination at: [7 6]\n",
      "Episode finished in 7 steps\n",
      "Total reward: 99.82\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 10 ===\n",
      "Driver at: [2 0], Destination at: [5 7]\n",
      "Episode finished in 10 steps\n",
      "Total reward: 99.60\n",
      "Success: Yes\n"
     ]
    }
   ],
   "source": [
    "# Visualize testing progress\n",
    "agent.visualize_testing_progess(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b57ba57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: 8×8 grid, 8 obstacles\n",
      "Agent: α=0.1, γ=0.95, ε: 1.0→0.01\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 20000\n",
    "env = MapEnv(render_mode=None)\n",
    "\n",
    "agent = DriverAgent(\n",
    "    env=env,\n",
    "    learning_rate=0.1,\n",
    "    initial_epsilon=1.0,\n",
    "    epsilon_decay=1e-5,\n",
    "    final_epsilon=0.01,\n",
    "    discount_factor=0.95,\n",
    ")\n",
    "\n",
    "print(f\"Environment: {env.size}×{env.size} grid, {len(env._obstacles)} obstacles\")\n",
    "print(f\"Agent: α=0.1, γ=0.95, ε: 1.0→0.01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "564f1ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "\n",
      "\n",
      "Training for 20000 episodes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/20000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DriverAgent.update() takes 6 positional arguments but 7 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m train_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining success rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverall_success_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m visualize_training(train_results, agent, n_episodes, window\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, algo_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQ-Learning\u001b[39m\u001b[38;5;124m'\u001b[39m, save_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq_learning_training_metrics.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[21], line 25\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(agent, env, n_episodes, log_interval)\u001b[0m\n\u001b[0;32m     22\u001b[0m next_action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(next_obs)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m#update using actual next action\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterminated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     28\u001b[0m steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mTypeError\u001b[0m: DriverAgent.update() takes 6 positional arguments but 7 were given"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining...\\n\")\n",
    "\n",
    "train_results = train_agent(agent, env, n_episodes)\n",
    "print(f\"Training success rate: {train_results['overall_success_rate']:.1%}\")\n",
    "\n",
    "visualize_training(train_results, agent, n_episodes, window=100, algo_name='Q-Learning', save_path='q_learning_training_metrics.png')\n",
    "\n",
    "print(\"\\nTesting... \\n\")\n",
    "test_results = test_agent(agent, env, 10)\n",
    "\n",
    "visualize_testing_progress(agent, MapEnv, n_episodes=10)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
