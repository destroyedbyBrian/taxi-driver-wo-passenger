{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3479d93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import time\n",
    "from Map import MapEnv\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd8aa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        learning_rate: float,\n",
    "        initial_epsilon: float,\n",
    "        epsilon_decay: float,\n",
    "        final_epsilon: float,\n",
    "        discount_factor: float,\n",
    "    ):\n",
    "        self.env = env\n",
    "        # Q-table\n",
    "        self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = initial_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.final_epsilon = final_epsilon\n",
    "\n",
    "        self.training_error = []\n",
    "\n",
    "    def _obs_to_tuple(self, obs):\n",
    "        d = obs[\"driver\"]\n",
    "        dest = obs[\"destination\"]\n",
    "        return (int(d[0]), int(d[1]), int(dest[0]), int(dest[1]))\n",
    "\n",
    "    def choose_action(self, obs: tuple[int, int, bool]) -> int: \n",
    "        obs_tuple = self._obs_to_tuple(obs)\n",
    "        # Exploration\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        # Exploitation\n",
    "        else :\n",
    "            return int(np.argmax(self.q_values[obs_tuple]))\n",
    "\n",
    "    def update(\n",
    "        self,\n",
    "        obs: tuple[int, int, bool],\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        terminated: bool,\n",
    "        next_obs: tuple[int, int, bool],\n",
    "    ):\n",
    "        obs_tuple = self._obs_to_tuple(obs)\n",
    "        next_obs_tuple = self._obs_to_tuple(next_obs)\n",
    "        \n",
    "        # Decide next best action\n",
    "        future_q_value = (not terminated) * np.max(self.q_values[next_obs_tuple])\n",
    "\n",
    "        # Bellman Equation to update Q-value\n",
    "        destination = reward + self.discount_factor * future_q_value\n",
    "\n",
    "        # How wrong was our current estimate?\n",
    "        temporal_difference = destination - self.q_values[obs_tuple][action]\n",
    "\n",
    "        # Update q-value in direction of error\n",
    "        self.q_values[obs_tuple][action] = (\n",
    "            self.q_values[obs_tuple][action] + self.lr * temporal_difference\n",
    "        )\n",
    "\n",
    "        # Track learning progress\n",
    "        self.training_error.append(temporal_difference)\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.final_epsilon, self.epsilon - self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b67c774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_moving_avgs(arr, window, convolution_mode):\n",
    "        return np.convolve(\n",
    "            np.array(arr).flatten(),\n",
    "            np.ones(window),\n",
    "            mode=convolution_mode\n",
    "        ) / window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ffa5f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, env, n_episodes: int, log_interval: int=5000):\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    epsilon_history = []\n",
    "    success_rate_history = []\n",
    "\n",
    "    print(f\"\\nTraining for {n_episodes} episodes...\\n\")\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        while not done:\n",
    "            # 1. Choose action\n",
    "            action = agent.choose_action(obs)\n",
    "\n",
    "            # 2. Take action\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "            # 3. Learn from experience & update Q-values\n",
    "            agent.update(obs, action, reward, terminated, next_obs)\n",
    "\n",
    "            # 4. Move to next state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "            obs = next_obs\n",
    "        # Take less random actions over time\n",
    "        agent.decay_epsilon()\n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        epsilon_history.append(agent.epsilon)\n",
    "\n",
    "        if episode >= 99:\n",
    "            recent_successes = [1 if r > 0 else 0 for r in episode_rewards[-100:]]\n",
    "            success_rate_history.append(np.mean(recent_successes))\n",
    "\n",
    "        if (episode + 1) % log_interval == 0:\n",
    "            recent_rewards = episode_rewards[-1000:]\n",
    "            tqdm.write(f\"\\nEpisode {episode + 1}/{n_episodes}\")\n",
    "            tqdm.write(f\"  Avg Reward: {np.mean(recent_rewards):.2f}\")\n",
    "            tqdm.write(f\"  Success Rate: {np.mean(recent_successes):.1%}\")\n",
    "            tqdm.write(f\"  Avg Steps: {np.mean(episode_lengths[-1000:]):.1f}\")\n",
    "            tqdm.write(f\"  Epsilon: {agent.epsilon:.4f}\")\n",
    "            tqdm.write(f\"  States: {len(agent.q_values)}\")\n",
    "\n",
    "    overall_success_rate = np.mean([1 if r > 0 else 0 for r in episode_rewards])\n",
    "\n",
    "    print(f\"\\nTraining complete. States explored: {len(agent.q_values)}\")\n",
    "    print(f\"Final epsilon: {agent.epsilon:.4f}\")\n",
    "    print(f\"Overall Success Rate: {overall_success_rate:.1%}\")\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"episode_rewards\": episode_rewards,\n",
    "        \"episode_lengths\": episode_lengths,\n",
    "        \"epsilon_history\": epsilon_history,\n",
    "        \"success_rate_history\": success_rate_history,\n",
    "        \"overall_success_rate\": overall_success_rate,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95f791fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_training(results, agent, n_episodes, window=100, algo_name='QLEARNING', save_path=None):\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "    matplotlib.use('TkAgg')  # or 'Qt5Agg' or 'Agg' depending on your system\n",
    "    episode_rewards = results[\"episode_rewards\"]\n",
    "    episode_lengths = results[\"episode_lengths\"]\n",
    "    success_rate_history = results[\"success_rate_history\"]\n",
    "    epsilon_history = results.get(\"epsilon_history\", [])\n",
    "    total_time = results.get(\"total_time\", None)\n",
    "    avg_time = results.get(\"avg_time_per_episode\", None)\n",
    "    td_errors = getattr(agent, \"training_error\", [])\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle(f'{algo_name} Training Metrics', fontsize=16, fontweight='bold')\n",
    "    def get_moving_avgs(data, window=100, mode='valid'):\n",
    "        return np.convolve(data, np.ones(window) / window, mode=mode)\n",
    "    # 1. Episode Rewards\n",
    "    ax = axes[0, 0]\n",
    "    rewards_ma = get_moving_avgs(episode_rewards, window, 'valid')\n",
    "    ax.plot(episode_rewards, alpha=0.2, color='blue', label='Raw')\n",
    "    ax.plot(range(window-1, len(episode_rewards)), rewards_ma,\n",
    "            color='blue', linewidth=2, label=f'{window}-ep MA')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Total Reward')\n",
    "    ax.set_title('Episode Rewards')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # 2. Success Rate\n",
    "    ax = axes[0, 1]\n",
    "    ax.plot(range(window-1, window-1 + len(success_rate_history)),\n",
    "            np.array(success_rate_history) * 100,\n",
    "            color='orange', linewidth=2.5, label=f'Rolling Success Rate ({window}ep)')\n",
    "    ax.axhline(y=100, color='gray', linestyle='--', alpha=0.5, linewidth=1, label='Perfect (100%)')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Success Rate (%)')\n",
    "    ax.set_title('Success Rate Over Time')\n",
    "    ax.set_ylim(0, 105)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if len(success_rate_history) > 0:\n",
    "        final_success = success_rate_history[-1] * 100\n",
    "        ax.text(0.98, 0.02, f'Final: {final_success:.1f}%',\n",
    "                transform=ax.transAxes, ha='right', va='bottom',\n",
    "                fontsize=10, fontweight='bold',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    # 3. Steps per Episode\n",
    "    ax = axes[0, 2]\n",
    "    lengths_ma = get_moving_avgs(episode_lengths, window, 'valid')\n",
    "    ax.plot(episode_lengths, alpha=0.2, color='green', label='Raw')\n",
    "    ax.plot(range(window-1, len(episode_lengths)), lengths_ma,\n",
    "            color='green', linewidth=2, label=f'{window}-ep MA')\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Steps')\n",
    "    ax.set_title('Steps per Episode')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    # 4. TD Errors\n",
    "    ax = axes[1, 0]\n",
    "    if len(td_errors) > 0:\n",
    "        if len(td_errors) > window:\n",
    "            td_ma = np.convolve(td_errors, np.ones(window)/window, mode='valid')\n",
    "            ax.plot(td_errors, alpha=0.2, color='red', label='Raw')\n",
    "            ax.plot(range(window-1, len(td_errors)), td_ma,\n",
    "                    color='red', linewidth=2, label=f'{window}-ep MA')\n",
    "        else:\n",
    "            ax.plot(td_errors, color='red')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Mean |TD Error|')\n",
    "        ax.set_title('Learning Stability (TD Errors)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No TD error data\", ha='center', va='center', fontsize=10, color='gray')\n",
    "        ax.axis('off')\n",
    "    # 5. Epsilon Decay\n",
    "    ax = axes[1, 1]\n",
    "    if len(epsilon_history) > 0:\n",
    "        ax.plot(epsilon_history, color='orange', linewidth=2)\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Epsilon (ε)')\n",
    "        ax.set_title('Exploration Rate Decay')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(-0.05, 1.05)\n",
    "    else:\n",
    "        ax.text(0.5, 0.5, \"No epsilon history\", ha='center', va='center',\n",
    "                fontsize=10, color='gray')\n",
    "        ax.axis('off')\n",
    "    # 6. Cumulative Reward\n",
    "    ax = axes[1, 2]\n",
    "    cumulative_rewards = np.cumsum(episode_rewards)\n",
    "    ax.plot(cumulative_rewards, color='purple', linewidth=2)\n",
    "    ax.set_xlabel('Episode')\n",
    "    ax.set_ylabel('Cumulative Reward')\n",
    "    ax.set_title('Cumulative Reward Over Episodes')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. Training Efficiency - Add BEFORE tight_layout\n",
    "    n_episodes = len(results[\"episode_rewards\"])\n",
    "    \n",
    "    if total_time is not None:\n",
    "        avg_time = total_time / n_episodes\n",
    "        efficiency_text = (\n",
    "            f\"Training Efficiency  |  \"\n",
    "            f\"Total Time: {total_time:.2f}s  |  \"\n",
    "            f\"Episodes: {n_episodes}  |  \"\n",
    "            f\"Avg/Episode: {avg_time:.4f}s\"\n",
    "        )\n",
    "        fig.text(\n",
    "            0.5, 0.015, efficiency_text,\n",
    "            ha='center', va='bottom', fontsize=10,\n",
    "            bbox=dict(boxstyle='round', facecolor='lavender', alpha=0.5, pad=0.6)\n",
    "        )\n",
    "    else:\n",
    "        fig.text(\n",
    "            0.5, 0.015, \"Training Efficiency: Timing data not available\",\n",
    "            ha='center', va='bottom', fontsize=10, color='gray',\n",
    "            bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.3, pad=0.6)\n",
    "        )\n",
    "    \n",
    "    # Call tight_layout AFTER adding the text, with proper spacing\n",
    "    plt.tight_layout(rect=[0, 0.04, 1, 0.97])\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Plot saved to {save_path}\")\n",
    "    else:\n",
    "        filename = f'{algo_name.lower()}_training_metrics.png'\n",
    "        plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a65e1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_agent(agent, env, n_episodes: int = 100, success_threshold: float = 0):\n",
    "\n",
    "    total_rewards = []\n",
    "    successes = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    old_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0  # no exploration\n",
    "\n",
    "    for _ in tqdm(range(n_episodes), desc=\"Testing\"):\n",
    "        obs, info = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        total_rewards.append(episode_reward)\n",
    "        successes.append(1 if episode_reward > success_threshold else 0)\n",
    "        episode_lengths.append(steps)\n",
    "\n",
    "    # restore epsilon\n",
    "    agent.epsilon = old_epsilon\n",
    "\n",
    "    results = {\n",
    "        \"success_rate\": np.mean(successes),\n",
    "        \"avg_reward\": np.mean(total_rewards),\n",
    "        \"std_reward\": np.std(total_rewards),\n",
    "        \"avg_length\": np.mean(episode_lengths),\n",
    "        \"std_length\": np.std(episode_lengths),\n",
    "        \"rewards\": total_rewards,\n",
    "        \"lengths\": episode_lengths,\n",
    "    }\n",
    "\n",
    "    # Optional summary print\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TEST RESULTS ({n_episodes} episodes)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Success Rate: {results['success_rate']:.1%}\")\n",
    "    print(f\"Average Reward: {results['avg_reward']:.3f} ± {results['std_reward']:.3f}\")\n",
    "    print(f\"Average Episode Len: {results['avg_length']:.1f} ± {results['std_length']:.1f}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5df50c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_testing_progress(agent, env_class, n_episodes=10):\n",
    "    render_env = env_class(render_mode=\"human\")\n",
    "    old_epsilon = agent.epsilon\n",
    "    agent.epsilon = 0.0  # pure exploitation mode\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        obs, info = render_env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "\n",
    "        print(f\"\\n=== Episode {episode + 1} ===\")\n",
    "        print(f\"Driver at: {obs['driver']}, Destination at: {obs['destination']}\")\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(obs)\n",
    "            obs, reward, terminated, truncated, info = render_env.step(action)\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            done = terminated or truncated\n",
    "\n",
    "        print(f\"Episode finished in {steps} steps\")\n",
    "        print(f\"Total reward: {episode_reward:.2f}\")\n",
    "        print(f\"Success: {'Yes' if terminated else 'No'}\")\n",
    "\n",
    "    agent.epsilon = old_epsilon\n",
    "    render_env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9145796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: 8×8 grid, 8 obstacles\n",
      "Hyperparameters: \n",
      "  Learning Rate: 0.15\n",
      "  Discount Factor: 0.95\n",
      "  Epsilon: start=1.0, end=0.008, decay=0.00015\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 20000\n",
    "env = MapEnv(render_mode=None)\n",
    "\n",
    "initial_eps = 1.0\n",
    "final_eps = 0.008\n",
    "epsilon_decay = (initial_eps - final_eps) / n_episodes\n",
    "\n",
    "agent = QLearningAgent(\n",
    "    env=env,\n",
    "    learning_rate=0.15,\n",
    "    initial_epsilon=initial_eps,\n",
    "    epsilon_decay=1.5e-4,\n",
    "    final_epsilon=final_eps,\n",
    "    discount_factor=0.95,\n",
    ")\n",
    "\n",
    "print(f\"Environment: {env.size}×{env.size} grid, {len(env._obstacles)} obstacles\")\n",
    "print(f\"Hyperparameters: \")\n",
    "print(f\"  Learning Rate: {agent.lr}\")\n",
    "print(f\"  Discount Factor: {agent.discount_factor}\")\n",
    "print(f\"  Epsilon: start={agent.epsilon}, end={agent.final_epsilon}, decay={agent.epsilon_decay}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae97c9f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training...\n",
      "\n",
      "\n",
      "Training for 20000 episodes...\n",
      "\n",
      "\n",
      "Episode 5000/20000\n",
      "  Avg Reward: 96.33\n",
      "  Success Rate: 100.0%\n",
      "  Avg Steps: 9.7\n",
      "  Epsilon: 0.2500\n",
      "  States: 3136\n",
      "\n",
      "Episode 10000/20000\n",
      "  Avg Reward: 99.80\n",
      "  Success Rate: 100.0%\n",
      "  Avg Steps: 6.0\n",
      "  Epsilon: 0.0080\n",
      "  States: 3136\n",
      "\n",
      "Episode 15000/20000\n",
      "  Avg Reward: 99.77\n",
      "  Success Rate: 100.0%\n",
      "  Avg Steps: 5.9\n",
      "  Epsilon: 0.0080\n",
      "  States: 3136\n",
      "\n",
      "Episode 20000/20000\n",
      "  Avg Reward: 99.80\n",
      "  Success Rate: 100.0%\n",
      "  Avg Steps: 6.0\n",
      "  Epsilon: 0.0080\n",
      "  States: 3136\n",
      "\n",
      "Training complete. States explored: 3136\n",
      "Final epsilon: 0.0080\n",
      "Overall Success Rate: 97.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining...\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "train_results = train_agent(agent, env, n_episodes)\n",
    "train_results[\"total_time\"] = time.time() - start_time\n",
    "train_results[\"avg_time_per_episode\"] = train_results[\"total_time\"] / n_episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f104cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plot saved to qlearning_training_metrics.png\n"
     ]
    }
   ],
   "source": [
    "visualize_training(train_results, agent, n_episodes, window=100, algo_name='QLEARNING', save_path='qlearning_training_metrics.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "412ad07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing... \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 10/10 [00:00<00:00, 831.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST RESULTS (10 episodes)\n",
      "============================================================\n",
      "Success Rate: 100.0%\n",
      "Average Reward: 99.952 ± 0.044\n",
      "Average Episode Len: 3.5 ± 1.8\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Episode 1 ===\n",
      "Driver at: [4 2], Destination at: [2 3]\n",
      "Episode finished in 3 steps\n",
      "Total reward: 99.98\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 2 ===\n",
      "Driver at: [1 0], Destination at: [4 2]\n",
      "Episode finished in 5 steps\n",
      "Total reward: 99.92\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 3 ===\n",
      "Driver at: [1 2], Destination at: [5 7]\n",
      "Episode finished in 9 steps\n",
      "Total reward: 99.73\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 4 ===\n",
      "Driver at: [3 2], Destination at: [0 5]\n",
      "Episode finished in 6 steps\n",
      "Total reward: 99.87\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 5 ===\n",
      "Driver at: [6 0], Destination at: [0 3]\n",
      "Episode finished in 9 steps\n",
      "Total reward: 99.72\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 6 ===\n",
      "Driver at: [2 4], Destination at: [5 3]\n",
      "Episode finished in 6 steps\n",
      "Total reward: 99.88\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 7 ===\n",
      "Driver at: [0 3], Destination at: [5 6]\n",
      "Episode finished in 8 steps\n",
      "Total reward: 99.78\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 8 ===\n",
      "Driver at: [7 1], Destination at: [5 1]\n",
      "Episode finished in 4 steps\n",
      "Total reward: 99.95\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 9 ===\n",
      "Driver at: [3 5], Destination at: [0 5]\n",
      "Episode finished in 3 steps\n",
      "Total reward: 99.97\n",
      "Success: Yes\n",
      "\n",
      "=== Episode 10 ===\n",
      "Driver at: [7 6], Destination at: [6 7]\n",
      "Episode finished in 2 steps\n",
      "Total reward: 99.99\n",
      "Success: Yes\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTesting... \\n\")\n",
    "test_results = test_agent(agent, env, 10)\n",
    "\n",
    "visualize_testing_progress(agent, MapEnv, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16e12707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "SARSA SUMMARY\n",
      "======================================================================\n",
      "Training Episodes:  20000\n",
      "States Explored:    3136\n",
      "Total Updates:      234251\n",
      "Final Epsilon:      0.00800\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"SARSA SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Training Episodes:  {n_episodes:}\")\n",
    "print(f\"States Explored:    {len(agent.q_values):}\")\n",
    "print(f\"Total Updates:      {len(agent.training_error):}\")\n",
    "print(f\"Final Epsilon:      {agent.epsilon:.5f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
